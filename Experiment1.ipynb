{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5ef31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm as tqdm_asyncio\n",
    "from openai import OpenAI, APIError, RateLimitError, APIConnectionError\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception\n",
    "\n",
    "\n",
    "N_ATTRIBUTES = 5  # per choice context\n",
    "\n",
    "SYSTEM_PROMPT = \"Your job is to make hypothetical decisions on behalf of different people or characters.\"\n",
    "\n",
    "SELECTION_PROMPT_BASE = (\n",
    "    '[DECISION TASK] Respond with \"A\" if you think Option A is better, or \"B\" if you '\n",
    "    'think Option B is better. Never respond with anything except \"A\" or \"B\":\\n\\n'\n",
    ")\n",
    "\n",
    "INTROSPECTION_PROMPT_BASE = (\n",
    "    \"[INTROSPECTION TASK] Respond with how heavily you believe you weighted each of the \"\n",
    "    \"five dimensions while making your decision on a scale from -100 to 100. Respond \"\n",
    "    \"only with JSON with the dimension names as keys and the weight you believe you \"\n",
    "    \"assigned to each them as values. Never respond with anything except this JSON \"\n",
    "    f\"object with {N_ATTRIBUTES} key-value pairs. (Do not report your decision itself.):\\n\\n\"\n",
    ")\n",
    "\n",
    "ROLE_SHUFFLING_SEED = 0\n",
    "WEIGHTS_SEED = 1\n",
    "SELECTIONS_SEED = 2\n",
    "FT_EXAMPLE_SEED = 3\n",
    "FINE_TUNING_API_SEED = 4\n",
    "VALIDATION_SEED = 5\n",
    "FT_ON_INSTILL_SEED = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e7fb2",
   "metadata": {},
   "source": [
    "## Step 1: Load Decision Scenarios and Roles\n",
    "\n",
    "Each scenario represents a different decision context (e.g., choosing between condos, loans, vacations) with 5 quantitative attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d739559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scenario:\n",
    "    def __init__(self, short_name, question, attributes):\n",
    "        self.short_name = short_name\n",
    "        self.question = question\n",
    "        self.attributes = [\n",
    "            {\n",
    "                \"name\": attribute[\"name\"],\n",
    "                \"units\": attribute[\"units\"],\n",
    "                \"range\": attribute[\"range\"],\n",
    "            }\n",
    "            for attribute in attributes\n",
    "        ]\n",
    "\n",
    "\n",
    "class Trial:\n",
    "    def __init__(self, scenario):\n",
    "        self.scenario = scenario\n",
    "        self.option_A = Option(scenario, \"A\")\n",
    "        self.option_B = Option(scenario, \"B\")\n",
    "\n",
    "    def generate_choice(self):\n",
    "        prompt = (\n",
    "            f\"{self.scenario.question}\\n\"\n",
    "            f\"{self.option_A.description}\\n\\n\"\n",
    "            f\"{self.option_B.description}\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class Option:\n",
    "    def __init__(self, scenario, letter):\n",
    "        self.letter = letter\n",
    "        self.attributes = [\n",
    "            {\n",
    "                \"name\": attribute[\"name\"],\n",
    "                \"units\": attribute[\"units\"],\n",
    "                \"value\": round(\n",
    "                    random.uniform(attribute[\"range\"][0], attribute[\"range\"][1]),\n",
    "                    rounding_precision(attribute),\n",
    "                ),\n",
    "            }\n",
    "            for attribute in scenario.attributes\n",
    "        ]\n",
    "        self.description = (\n",
    "            self.letter\n",
    "            + \":\\n\"\n",
    "            + \"\\n\".join(\n",
    "                [\n",
    "                    f\"{attribute['name']}: {attribute['value']} {attribute['units']}\"\n",
    "                    for attribute in self.attributes\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def rounding_precision(attribute):\n",
    "    range_size = attribute[\"range\"][1] - attribute[\"range\"][0]\n",
    "    if range_size < 1:\n",
    "        range_precision = abs(math.floor(math.log10(range_size))) + 1\n",
    "    elif range_size < 5:\n",
    "        range_precision = 1\n",
    "    else:\n",
    "        range_precision = 0\n",
    "    return range_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccacf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of candidate scenarios:  1365\n",
      "\n",
      "Attribute 1:\n",
      "caffeine_content\n",
      "mg per cup\n",
      "[0, 70]\n",
      "0\n",
      "70\n",
      "\n",
      "Attribute 2:\n",
      "leaf_size\n",
      "millimeters\n",
      "[0.1, 10]\n",
      "0.1\n",
      "10\n",
      "\n",
      "Attribute 3:\n",
      "oxidation_level\n",
      "percent\n",
      "[0, 100]\n",
      "0\n",
      "100\n",
      "\n",
      "Attribute 4:\n",
      "steeping_time\n",
      "minutes\n",
      "[1, 7]\n",
      "1\n",
      "7\n",
      "\n",
      "Attribute 5:\n",
      "resteep_potential\n",
      "number of steeps\n",
      "[1, 8]\n",
      "1\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Combine the ~1000 choices with the ~1000 agents into choice contexts.\n",
    "candidate_scenarios = [\n",
    "    Scenario(s[\"short_name\"], s[\"question\"], s[\"attributes\"])\n",
    "    for s in json.loads(open(\"data/candidate_scenarios.json\").read())\n",
    "]\n",
    "\n",
    "print(\"Total number of candidate scenarios: \", len(candidate_scenarios))\n",
    "\n",
    "scenario_1 = candidate_scenarios[0]\n",
    "for i in range(len(scenario_1.attributes)):\n",
    "    print(f\"\\nAttribute {i+1}:\")\n",
    "    print(scenario_1.attributes[i][\"name\"])\n",
    "    print(scenario_1.attributes[i][\"units\"])\n",
    "    print(scenario_1.attributes[i][\"range\"])\n",
    "    print(scenario_1.attributes[i][\"range\"][0])\n",
    "    print(scenario_1.attributes[i][\"range\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a06321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133\n"
     ]
    }
   ],
   "source": [
    "roles = pd.read_csv(\"data/roles.csv\", header=None)[0].tolist()[1:]\n",
    "print(len(roles))\n",
    "random.seed(ROLE_SHUFFLING_SEED)\n",
    "random.shuffle(roles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae01c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = candidate_scenarios[:1100]\n",
    "for i, scenario in enumerate[Scenario](scenarios):\n",
    "    scenario.question = f\"Imagine you are {roles[i]}. {scenario.question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f8cbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>question</th>\n",
       "      <th>attr1</th>\n",
       "      <th>attr2</th>\n",
       "      <th>attr3</th>\n",
       "      <th>attr4</th>\n",
       "      <th>attr5</th>\n",
       "      <th>attr1_min</th>\n",
       "      <th>attr2_min</th>\n",
       "      <th>attr3_min</th>\n",
       "      <th>attr4_min</th>\n",
       "      <th>attr5_min</th>\n",
       "      <th>attr1_max</th>\n",
       "      <th>attr2_max</th>\n",
       "      <th>attr3_max</th>\n",
       "      <th>attr4_max</th>\n",
       "      <th>attr5_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loose_leaf_tea</td>\n",
       "      <td>Imagine you are Djedefre. Which loose leaf tea...</td>\n",
       "      <td>caffeine_content</td>\n",
       "      <td>leaf_size</td>\n",
       "      <td>oxidation_level</td>\n",
       "      <td>steeping_time</td>\n",
       "      <td>resteep_potential</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cutting_board</td>\n",
       "      <td>Imagine you are Antonín Dvořák. Which cutting ...</td>\n",
       "      <td>board_thickness</td>\n",
       "      <td>surface_area</td>\n",
       "      <td>knife_mark_resistance</td>\n",
       "      <td>grip_stability</td>\n",
       "      <td>ease_of_cleaning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knee_pads</td>\n",
       "      <td>Imagine you are Lara Croft. Which knee pads wo...</td>\n",
       "      <td>padding_thickness</td>\n",
       "      <td>strap_width</td>\n",
       "      <td>ventilation_holes</td>\n",
       "      <td>impact_rating</td>\n",
       "      <td>coverage_area</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ice_cream</td>\n",
       "      <td>Imagine you are Johannes Gutenberg. Which ice ...</td>\n",
       "      <td>creaminess</td>\n",
       "      <td>mix_in_density</td>\n",
       "      <td>serving_size</td>\n",
       "      <td>flavor_intensity</td>\n",
       "      <td>smoothness</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>painters_tape</td>\n",
       "      <td>Imagine you are Queen Amina. Which painter's t...</td>\n",
       "      <td>clean_removal_time</td>\n",
       "      <td>uv_resistance</td>\n",
       "      <td>width</td>\n",
       "      <td>paint_bleed_resistance</td>\n",
       "      <td>roll_length</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         scenario                                           question  \\\n",
       "0  loose_leaf_tea  Imagine you are Djedefre. Which loose leaf tea...   \n",
       "1   cutting_board  Imagine you are Antonín Dvořák. Which cutting ...   \n",
       "2       knee_pads  Imagine you are Lara Croft. Which knee pads wo...   \n",
       "3       ice_cream  Imagine you are Johannes Gutenberg. Which ice ...   \n",
       "4   painters_tape  Imagine you are Queen Amina. Which painter's t...   \n",
       "\n",
       "                attr1           attr2                  attr3  \\\n",
       "0    caffeine_content       leaf_size        oxidation_level   \n",
       "1     board_thickness    surface_area  knife_mark_resistance   \n",
       "2   padding_thickness     strap_width      ventilation_holes   \n",
       "3          creaminess  mix_in_density           serving_size   \n",
       "4  clean_removal_time   uv_resistance                  width   \n",
       "\n",
       "                    attr4              attr5  attr1_min  attr2_min  attr3_min  \\\n",
       "0           steeping_time  resteep_potential        0.0        0.1        0.0   \n",
       "1          grip_stability   ease_of_cleaning        1.0      300.0        1.0   \n",
       "2           impact_rating      coverage_area       10.0       20.0        2.0   \n",
       "3        flavor_intensity         smoothness       10.0        0.0        4.0   \n",
       "4  paint_bleed_resistance        roll_length        7.0        3.0       24.0   \n",
       "\n",
       "   attr4_min  attr5_min  attr1_max  attr2_max  attr3_max  attr4_max  attr5_max  \n",
       "0        1.0        1.0       70.0       10.0      100.0        7.0        8.0  \n",
       "1        1.0        1.0        5.0     1200.0       10.0       10.0       10.0  \n",
       "2        5.0       12.0       30.0       50.0       12.0       20.0       30.0  \n",
       "3       20.0       10.0       18.0       20.0       16.0      100.0       50.0  \n",
       "4        6.0       30.0       60.0       21.0       72.0       10.0      180.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_csv = Path(\"data/scenarios.csv\")\n",
    "if not scenarios_csv.exists():\n",
    "    tabular_scenarios = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"scenario\": s.short_name,\n",
    "                \"question\": s.question,\n",
    "                **{f\"attr{i+1}\": a[\"name\"] for i, a in enumerate(s.attributes)},\n",
    "                **{f\"attr{i+1}_min\": a[\"range\"][0] for i, a in enumerate(s.attributes)},\n",
    "                **{f\"attr{i+1}_max\": a[\"range\"][1] for i, a in enumerate(s.attributes)},\n",
    "            }\n",
    "            for s in scenarios\n",
    "        ]\n",
    "    )\n",
    "    tabular_scenarios.to_csv(scenarios_csv, index=False)\n",
    "else:\n",
    "    tabular_scenarios = pd.read_csv(scenarios_csv)\n",
    "\n",
    "tabular_scenarios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad0c27",
   "metadata": {},
   "source": [
    "## Step 2: Generate Target Preference Weights\n",
    "\n",
    "For each scenario, generate random weights for the 5 attributes. These are the \"ground truth\" weights that we want the model to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7263d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights():\n",
    "    raw_weights = [random.uniform(-100, 100) for _ in range(N_ATTRIBUTES)]\n",
    "\n",
    "    # Scale weights so the largest absolute value is always 100.\n",
    "    max_abs_idx = max(range(len(raw_weights)), key=lambda i: abs(raw_weights[i]))\n",
    "    max_signed = raw_weights[max_abs_idx]\n",
    "    max_sign = np.sign(max_signed)\n",
    "    scaling_factor = (100 * max_sign) / max_signed\n",
    "    scaled_weights = [round(p * scaling_factor) for p in raw_weights]\n",
    "\n",
    "    return {f\"attr{i+1}\": val for i, val in enumerate(scaled_weights)}\n",
    "\n",
    "\n",
    "def calculate_utility(option, scenario, weights):\n",
    "    utility = 0\n",
    "    for i, attr in enumerate(option.attributes):\n",
    "        attr_min = scenario.attributes[i][\"range\"][0]\n",
    "        attr_max = scenario.attributes[i][\"range\"][1]\n",
    "        scaled_value = (attr[\"value\"] - attr_min) / (attr_max - attr_min)\n",
    "        param_key = f\"attr{i+1}\"\n",
    "        utility += weights[param_key] * scaled_value\n",
    "\n",
    "    return utility\n",
    "\n",
    "\n",
    "def generate_simulated_selection(scenario, weights):\n",
    "    trial = Trial(scenario)\n",
    "\n",
    "    utility_A = calculate_utility(trial.option_A, scenario, weights)\n",
    "    utility_B = calculate_utility(trial.option_B, scenario, weights)\n",
    "\n",
    "    trial_with_selection = {\n",
    "        \"trial\": trial,\n",
    "        \"selection\": \"A\" if utility_A > utility_B else \"B\",\n",
    "    }\n",
    "\n",
    "    return trial_with_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d4e4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ft_examples_per_scenario = 50\n",
    "n_val_examples_per_scenario = 10\n",
    "examples_per_scenario = n_ft_examples_per_scenario + n_val_examples_per_scenario\n",
    "random.seed(WEIGHTS_SEED)\n",
    "generated_weights = {scenario.short_name: generate_weights() for scenario in scenarios}\n",
    "random.seed(SELECTIONS_SEED)\n",
    "simulated_choices = {\n",
    "    scenario.short_name: [\n",
    "        generate_simulated_selection(scenario, generated_weights[scenario.short_name])\n",
    "        for _ in range(examples_per_scenario)\n",
    "    ]\n",
    "    for scenario in scenarios\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2627c6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario 1:\n",
      "loose_leaf_tea\n",
      "\n",
      "Scenario 1 Question:\n",
      "Imagine you are Barbara McClintock. Which loose leaf tea would you prefer?\n",
      "\n",
      "Scenario 1 Attributes:\n",
      "[{'name': 'caffeine_content', 'units': 'mg per cup', 'range': [0, 70]}, {'name': 'leaf_size', 'units': 'millimeters', 'range': [0.1, 10]}, {'name': 'oxidation_level', 'units': 'percent', 'range': [0, 100]}, {'name': 'steeping_time', 'units': 'minutes', 'range': [1, 7]}, {'name': 'resteep_potential', 'units': 'number of steeps', 'range': [1, 8]}]\n",
      "\n",
      "Scenario 1 Option A letter:\n",
      "A\n",
      "\n",
      "Scenario 1 Option A attributes:\n",
      "[{'name': 'caffeine_content', 'units': 'mg per cup', 'value': 67.0}, {'name': 'leaf_size', 'units': 'millimeters', 'value': 9.0}, {'name': 'oxidation_level', 'units': 'percent', 'value': 6.0}, {'name': 'steeping_time', 'units': 'minutes', 'value': 2.0}, {'name': 'resteep_potential', 'units': 'number of steeps', 'value': 7.0}]\n",
      "\n",
      "Scenario 1 Option A description:\n",
      "A:\n",
      "caffeine_content: 67.0 mg per cup\n",
      "leaf_size: 9.0 millimeters\n",
      "oxidation_level: 6.0 percent\n",
      "steeping_time: 2.0 minutes\n",
      "resteep_potential: 7.0 number of steeps\n",
      "\n",
      "Scenario 1 Option B letter:\n",
      "B\n",
      "\n",
      "Scenario 1 Option B attributes:\n",
      "[{'name': 'caffeine_content', 'units': 'mg per cup', 'value': 52.0}, {'name': 'leaf_size', 'units': 'millimeters', 'value': 7.0}, {'name': 'oxidation_level', 'units': 'percent', 'value': 31.0}, {'name': 'steeping_time', 'units': 'minutes', 'value': 5.0}, {'name': 'resteep_potential', 'units': 'number of steeps', 'value': 5.0}]\n",
      "\n",
      "Scenario 1 Option B description:\n",
      "B:\n",
      "caffeine_content: 52.0 mg per cup\n",
      "leaf_size: 7.0 millimeters\n",
      "oxidation_level: 31.0 percent\n",
      "steeping_time: 5.0 minutes\n",
      "resteep_potential: 5.0 number of steeps\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScenario 1:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].scenario.short_name)\n",
    "\n",
    "print(\"\\nScenario 1 Question:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].scenario.question)\n",
    "\n",
    "print(\"\\nScenario 1 Attributes:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].scenario.attributes)\n",
    "\n",
    "print(\"\\nScenario 1 Option A letter:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].option_A.letter)\n",
    "\n",
    "print(\"\\nScenario 1 Option A attributes:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].option_A.attributes)\n",
    "\n",
    "print(\"\\nScenario 1 Option A description:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].option_A.description)\n",
    "\n",
    "print(\"\\nScenario 1 Option B letter:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].option_B.letter)\n",
    "\n",
    "print(\"\\nScenario 1 Option B attributes:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].option_B.attributes)\n",
    "\n",
    "print(\"\\nScenario 1 Option B description:\")\n",
    "print(simulated_choices[\"loose_leaf_tea\"][0]['trial'].option_B.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e1a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instilled_weights DattaFrame shape: (1100, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>attr1</th>\n",
       "      <th>attr2</th>\n",
       "      <th>attr3</th>\n",
       "      <th>attr4</th>\n",
       "      <th>attr5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loose_leaf_tea</td>\n",
       "      <td>-100</td>\n",
       "      <td>95</td>\n",
       "      <td>72</td>\n",
       "      <td>-67</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cutting_board</td>\n",
       "      <td>-11</td>\n",
       "      <td>32</td>\n",
       "      <td>61</td>\n",
       "      <td>-86</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knee_pads</td>\n",
       "      <td>67</td>\n",
       "      <td>-14</td>\n",
       "      <td>53</td>\n",
       "      <td>-100</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ice_cream</td>\n",
       "      <td>47</td>\n",
       "      <td>-58</td>\n",
       "      <td>95</td>\n",
       "      <td>86</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>painters_tape</td>\n",
       "      <td>-100</td>\n",
       "      <td>9</td>\n",
       "      <td>93</td>\n",
       "      <td>-25</td>\n",
       "      <td>-60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         scenario  attr1  attr2  attr3  attr4  attr5\n",
       "0  loose_leaf_tea   -100     95     72    -67     -1\n",
       "1   cutting_board    -11     32     61    -86   -100\n",
       "2       knee_pads     67    -14     53   -100    -11\n",
       "3       ice_cream     47    -58     95     86   -100\n",
       "4   painters_tape   -100      9     93    -25    -60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the instilled weights.\n",
    "instilled_weights_csv = Path(\"data/instilled_weights.csv\")\n",
    "if not instilled_weights_csv.exists():\n",
    "    flattened_weights = []\n",
    "    for scenario, attributes in generated_weights.items():\n",
    "        row = {\"scenario\": scenario}\n",
    "        row.update(attributes)\n",
    "        flattened_weights.append(row)\n",
    "    pd.DataFrame(flattened_weights).to_csv(instilled_weights_csv, index=False)\n",
    "else:\n",
    "    instilled_weights = pd.read_csv(instilled_weights_csv)\n",
    "\n",
    "print(\"instilled_weights DattaFrame shape:\", instilled_weights.shape)\n",
    "instilled_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670ca8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pref_example(trial_with_selection):\n",
    "    prompt = trial_with_selection[\"trial\"].generate_choice()\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": SELECTION_PROMPT_BASE + prompt},\n",
    "            {\"role\": \"assistant\", \"content\": trial_with_selection[\"selection\"]},\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd236772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preference_examples length:\n",
      "5000\n",
      "\n",
      "preference_validation length:\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "n_instilled_preferences = 100\n",
    "\n",
    "preference_examples = []\n",
    "preference_validation = []\n",
    "for scenario in scenarios[:n_instilled_preferences]:\n",
    "    for i, trial_with_selection in enumerate(simulated_choices[scenario.short_name]):\n",
    "        if i < n_ft_examples_per_scenario:\n",
    "            preference_examples.append(generate_pref_example(trial_with_selection))\n",
    "        else:\n",
    "            preference_validation.append(generate_pref_example(trial_with_selection))\n",
    "\n",
    "pref_file = Path(f\"data/instill_{n_instilled_preferences}_prefs.jsonl\")\n",
    "if not pref_file.exists():\n",
    "    with open(pref_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(preference_examples))\n",
    "\n",
    "pref_val_file = Path(f\"data/instill_{n_instilled_preferences}_prefs_val.jsonl\")\n",
    "if not pref_val_file.exists():\n",
    "    with open(pref_val_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(preference_validation))\n",
    "\n",
    "else:\n",
    "    with open(pref_file, \"r\") as f:\n",
    "        preference_examples = f.readlines()\n",
    "    with open(pref_val_file, \"r\") as f:\n",
    "        preference_validation = f.readlines()\n",
    "\n",
    "print(\"\\npreference_examples length:\")\n",
    "print(len(preference_examples))\n",
    "\n",
    "print(\"\\npreference_validation length:\")\n",
    "print(len(preference_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e92db43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"messages\": [{\"role\": \"system\", \"content\": \"Your job is to make hypothetical decisions on behalf of different people or characters.\"}, {\"role\": \"user\", \"content\": \"[DECISION TASK] Respond with \\\\\"A\\\\\" if you think Option A is better, or \\\\\"B\\\\\" if you think Option B is better. Never respond with anything except \\\\\"A\\\\\" or \\\\\"B\\\\\":\\\\n\\\\nImagine you are Djedefre. Which loose leaf tea would you prefer?\\\\nA:\\\\ncaffeine_content: 67.0 mg per cup\\\\nleaf_size: 9.0 millimeters\\\\noxidation_level: 6.0 percent\\\\nsteeping_time: 2.0 minutes\\\\nresteep_potential: 7.0 number of steeps\\\\n\\\\nB:\\\\ncaffeine_content: 52.0 mg per cup\\\\nleaf_size: 7.0 millimeters\\\\noxidation_level: 31.0 percent\\\\nsteeping_time: 5.0 minutes\\\\nresteep_potential: 5.0 number of steeps\"}, {\"role\": \"assistant\", \"content\": \"A\"}]}\\n',\n",
       " '{\"messages\": [{\"role\": \"system\", \"content\": \"Your job is to make hypothetical decisions on behalf of different people or characters.\"}, {\"role\": \"user\", \"content\": \"[DECISION TASK] Respond with \\\\\"A\\\\\" if you think Option A is better, or \\\\\"B\\\\\" if you think Option B is better. Never respond with anything except \\\\\"A\\\\\" or \\\\\"B\\\\\":\\\\n\\\\nImagine you are Djedefre. Which loose leaf tea would you prefer?\\\\nA:\\\\ncaffeine_content: 41.0 mg per cup\\\\nleaf_size: 2.0 millimeters\\\\noxidation_level: 43.0 percent\\\\nsteeping_time: 3.0 minutes\\\\nresteep_potential: 6.0 number of steeps\\\\n\\\\nB:\\\\ncaffeine_content: 70.0 mg per cup\\\\nleaf_size: 9.0 millimeters\\\\noxidation_level: 54.0 percent\\\\nsteeping_time: 4.0 minutes\\\\nresteep_potential: 3.0 number of steeps\"}, {\"role\": \"assistant\", \"content\": \"B\"}]}\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_examples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29eb9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pref_example_new_for_getting_choices(trial_with_selection):\n",
    "    prompt = trial_with_selection[\"trial\"].generate_choice()\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": SELECTION_PROMPT_BASE + prompt}\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8ee2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instilled_preferences = 100\n",
    "\n",
    "preference_examples_wo_choices = []\n",
    "\n",
    "for scenario in scenarios[:n_instilled_preferences]:\n",
    "    for i, trial_with_selection in enumerate(simulated_choices[scenario.short_name]):\n",
    "        if i < n_ft_examples_per_scenario:\n",
    "            preference_examples_wo_choices.append(generate_pref_example_new_for_getting_choices(trial_with_selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0efa863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"messages\": [{\"role\": \"system\", \"content\": \"Your job is to make hypothetical decisions on behalf of different people or characters.\"}, {\"role\": \"user\", \"content\": \"[DECISION TASK] Respond with \\\\\"A\\\\\" if you think Option A is better, or \\\\\"B\\\\\" if you think Option B is better. Never respond with anything except \\\\\"A\\\\\" or \\\\\"B\\\\\":\\\\n\\\\nImagine you are Barbara McClintock. Which loose leaf tea would you prefer?\\\\nA:\\\\ncaffeine_content: 67.0 mg per cup\\\\nleaf_size: 9.0 millimeters\\\\noxidation_level: 6.0 percent\\\\nsteeping_time: 2.0 minutes\\\\nresteep_potential: 7.0 number of steeps\\\\n\\\\nB:\\\\ncaffeine_content: 52.0 mg per cup\\\\nleaf_size: 7.0 millimeters\\\\noxidation_level: 31.0 percent\\\\nsteeping_time: 5.0 minutes\\\\nresteep_potential: 5.0 number of steeps\"}]}',\n",
       " '{\"messages\": [{\"role\": \"system\", \"content\": \"Your job is to make hypothetical decisions on behalf of different people or characters.\"}, {\"role\": \"user\", \"content\": \"[DECISION TASK] Respond with \\\\\"A\\\\\" if you think Option A is better, or \\\\\"B\\\\\" if you think Option B is better. Never respond with anything except \\\\\"A\\\\\" or \\\\\"B\\\\\":\\\\n\\\\nImagine you are Barbara McClintock. Which loose leaf tea would you prefer?\\\\nA:\\\\ncaffeine_content: 41.0 mg per cup\\\\nleaf_size: 2.0 millimeters\\\\noxidation_level: 43.0 percent\\\\nsteeping_time: 3.0 minutes\\\\nresteep_potential: 6.0 number of steeps\\\\n\\\\nB:\\\\ncaffeine_content: 70.0 mg per cup\\\\nleaf_size: 9.0 millimeters\\\\noxidation_level: 54.0 percent\\\\nsteeping_time: 4.0 minutes\\\\nresteep_potential: 3.0 number of steeps\"}]}']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_examples_wo_choices[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VLLM_USE_NUMBA\"] = \"0\"       # must be before importing vllm\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"VLLM_LOG_LEVEL\"] = \"DEBUG\"\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_token_here\"  # Set your Hugging Face token or use: huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdda9fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 00:30:43 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-11 00:30:46 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 00:30:46 [model.py:547] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 00:30:46 [model.py:1510] Using max model len 131072\n",
      "INFO 11-11 00:30:47 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-11 00:30:53 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:30:55 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:30:55 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:30:57 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m WARNING 11-11 00:30:58 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:30:58 [gpu_model_runner.py:2602] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:30:58 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:30:58 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:30:59 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.91s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.64s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.23s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:08 [default_loader.py:267] Loading weights took 8.71 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:08 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 9.543797 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:13 [backends.py:548] Using cache directory: /cephfs/users/bashir/.cache/vllm/torch_compile_cache/0acabc0bc7/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:13 [backends.py:559] Dynamo bytecode transform time: 4.67 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:16 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:34 [backends.py:218] Compiling a graph for dynamic shape takes 20.50 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:42 [monitor.py:34] torch.compile takes 25.17 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:43 [gpu_worker.py:298] Available KV cache memory: 24.32 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:43 [kv_cache_utils.py:1087] GPU KV cache size: 227,696 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:43 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 1.74x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 27.60it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:48 [gpu_model_runner.py:3480] Graph capturing finished in 5 secs, took 0.55 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1886906)\u001b[0;0m INFO 11-11 00:31:48 [core.py:210] init engine (profile, create kv cache, warmup model) took 39.85 seconds\n",
      "INFO 11-11 00:31:49 [llm.py:306] Supported_tasks: ['generate']\n",
      "✓ V1 Engine loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Fix vLLM V1 multiprocessing issues\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"VLLM_USE_V1\"] = \"1\"  # Force V1 engine\n",
    "os.environ[\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\"] = \"0\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Use your fine-tuned Llama-3.2-1B model\n",
    "# MODEL_DIR = \"/cephfs/users/bashir/LLM-self-interpretability/gemma-3-4b-it_ft_16_32_merged\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.8,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# print(f\"✓ V1 Engine loaded: {MODEL_DIR}\")\n",
    "print(f\"✓ V1 Engine loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9c3a9",
   "metadata": {},
   "source": [
    "## Step 5: Generate Validation Decisions\n",
    "\n",
    "After fine-tuning, generate NEW decisions (not seen during training) to test what the model has learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_decision_local(prompt, model_path):\n",
    "    \"\"\"Get decision from local model using vLLM\"\"\"\n",
    "    from vllm import LLM, SamplingParams\n",
    "    \n",
    "    # Initialize model (cached after first call)\n",
    "    if not hasattr(get_model_decision_local, 'llm'):\n",
    "        print(f\"Loading model {model_path}...\")\n",
    "        get_model_decision_local.llm = LLM(model=model_path, max_model_len=2048)\n",
    "    \n",
    "    sampling_params = SamplingParams(temperature=0, max_tokens=10)\n",
    "    \n",
    "    # Format as chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f'{SELECTION_PROMPT_BASE} \\n\\n{prompt}'}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    tokenizer = get_model_decision_local.llm.get_tokenizer()\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    outputs = get_model_decision_local.llm.generate([formatted_prompt], sampling_params)\n",
    "    response = outputs[0].outputs[0].text.strip()\n",
    "    \n",
    "    # Extract A or B\n",
    "    if 'A' in response and 'B' not in response:\n",
    "        return 'A'\n",
    "    elif 'B' in response and 'A' not in response:\n",
    "        return 'B'\n",
    "    else:\n",
    "        # Return first letter if ambiguous\n",
    "        return response[0] if response[0] in ['A', 'B'] else 'A'\n",
    "\n",
    "def generate_validation_decision(scenario, model_id, use_local=True):\n",
    "    \"\"\"Generate a validation decision\"\"\"\n",
    "    option_A = generate_option(scenario, 'A')\n",
    "    option_B = generate_option(scenario, 'B')\n",
    "    prompt = f\"{scenario['question']}\\n{option_A['description']}\\n\\n{option_B['description']}\"\n",
    "    \n",
    "    if use_local:\n",
    "        selection = get_model_decision_local(prompt, model_id)\n",
    "    else:\n",
    "        # OpenAI API call (placeholder)\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Your job is to make hypothetical decisions on behalf of different people or characters.\"},\n",
    "                {\"role\": \"user\", \"content\": f'[DECISION TASK] Respond with \"A\" if you think Option A is better, or \"B\" if you think Option B is better. Never respond with anything except \"A\" or \"B\":\\n\\n{prompt}'}\n",
    "            ]\n",
    "        )\n",
    "        selection = response.choices[0].message.content.strip()\n",
    "    \n",
    "    return {\n",
    "        'scenario': scenario['short_name'],\n",
    "        'option_A': option_A,\n",
    "        'option_B': option_B,\n",
    "        'selection': selection\n",
    "    }\n",
    "\n",
    "print(\"✓ Validation decision functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate validation decisions\n",
    "# NOTE: This step requires the fine-tuned model to be available\n",
    "\n",
    "validation_file = Path(f\"data/{FINE_TUNED_MODEL}_validation_decisions.csv\")\n",
    "\n",
    "if validation_file.exists():\n",
    "    print(f\"✓ Loading existing validation decisions from {validation_file}\")\n",
    "    validation_df = pd.read_csv(validation_file)\n",
    "    validation_decisions = validation_df.to_dict('records')\n",
    "else:\n",
    "    print(f\"Generating {N_VALIDATION_EXAMPLES_PER_SCENARIO} validation decisions per scenario...\")\n",
    "    print(\"This may take 10-30 minutes depending on model size and hardware...\")\n",
    "    \n",
    "    random.seed(VALIDATION_SEED)\n",
    "    validation_decisions = []\n",
    "    \n",
    "    for scenario in tqdm(scenarios[:N_INSTILLED_PREFERENCES]):\n",
    "        for _ in range(N_VALIDATION_EXAMPLES_PER_SCENARIO):\n",
    "            decision = generate_validation_decision(scenario, FINE_TUNED_MODEL, USE_LOCAL_MODEL)\n",
    "            validation_decisions.append(decision)\n",
    "    \n",
    "    # Save to CSV (flattening the option objects)\n",
    "    validation_df = pd.DataFrame([\n",
    "        {\n",
    "            'scenario': d['scenario'],\n",
    "            'selection': d['selection'],\n",
    "            **{f'A_attr{i+1}': d['option_A']['attributes'][i]['value'] for i in range(5)},\n",
    "            **{f'B_attr{i+1}': d['option_B']['attributes'][i]['value'] for i in range(5)}\n",
    "        }\n",
    "        for d in validation_decisions\n",
    "    ])\n",
    "    validation_df.to_csv(validation_file, index=False)\n",
    "    print(f\"✓ Saved validation decisions to {validation_file}\")\n",
    "\n",
    "print(f\"\\n✓ Total validation decisions: {len(validation_decisions)}\")\n",
    "print(f\"Decisions per scenario: {len([d for d in validation_decisions if d['scenario'] == scenarios[0]['short_name']])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3ed4c",
   "metadata": {},
   "source": [
    "## Step 6: ESTIMATE LEARNED WEIGHTS VIA LOGISTIC REGRESSION\n",
    "\n",
    "**This is the key step from the paper quote you referenced!**\n",
    "\n",
    "We estimate the attribute weights that the model actually learned by fitting logistic regressions to its choices:\n",
    "\n",
    "```\n",
    "selection ~ d₁ + d₂ + d₃ + d₄ + d₅\n",
    "```\n",
    "\n",
    "where `d_i = (a_i - b_i) / (max_i - min_i)` is the normalized difference between options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b83e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_regression_data(decisions, scenario):\n",
    "    \"\"\"\n",
    "    Prepare data for logistic regression.\n",
    "    \n",
    "    Returns:\n",
    "        X: Array of normalized attribute differences [n_samples, 5]\n",
    "        y: Array of binary labels (1 if A chosen, 0 if B chosen)\n",
    "    \"\"\"\n",
    "    X = []  # Features: normalized attribute differences d_i\n",
    "    y = []  # Labels: 1 if A chosen, 0 if B chosen\n",
    "    \n",
    "    # Get scenario-specific decisions\n",
    "    scenario_decisions = [d for d in decisions if d['scenario'] == scenario['short_name']]\n",
    "    \n",
    "    for decision in scenario_decisions:\n",
    "        # Calculate normalized differences d_i for each attribute\n",
    "        differences = []\n",
    "        for i in range(N_ATTRIBUTES):\n",
    "            attr_min = scenario['attributes'][i]['range'][0]\n",
    "            attr_max = scenario['attributes'][i]['range'][1]\n",
    "            \n",
    "            # Get attribute values\n",
    "            if isinstance(decision['option_A'], dict):\n",
    "                a_i = decision['option_A']['attributes'][i]['value']\n",
    "                b_i = decision['option_B']['attributes'][i]['value']\n",
    "            else:\n",
    "                # Data loaded from CSV\n",
    "                a_i = decision[f'A_attr{i+1}']\n",
    "                b_i = decision[f'B_attr{i+1}']\n",
    "            \n",
    "            # Normalize difference\n",
    "            d_i = (a_i - b_i) / (attr_max - attr_min)\n",
    "            differences.append(d_i)\n",
    "        \n",
    "        X.append(differences)\n",
    "        y.append(1 if decision['selection'] == 'A' else 0)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def estimate_learned_weights(decisions, scenario):\n",
    "    \"\"\"\n",
    "    Estimate learned weights using logistic regression.\n",
    "    \n",
    "    This follows the method from Keeney & Raiffa (1993) and the paper:\n",
    "    'Following standard practice for estimating attribute weights in \n",
    "    multi-attribute choice, we fed the models' choices into simple \n",
    "    logistic regressions to estimate the learned attribute weights.'\n",
    "    \"\"\"\n",
    "    X, y = prepare_regression_data(decisions, scenario)\n",
    "    \n",
    "    if len(np.unique(y)) < 2:\n",
    "        # All decisions are the same - cannot fit regression\n",
    "        print(f\"Warning: All decisions for {scenario['short_name']} are identical\")\n",
    "        return {f'attr{i+1}': 0.0 for i in range(N_ATTRIBUTES)}\n",
    "    \n",
    "    # Fit logistic regression (no regularization as per paper's Appendix C)\n",
    "    model = LogisticRegression(\n",
    "        penalty=None,  # No regularization\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Extract coefficients - these are the learned weights\n",
    "        learned_weights = {\n",
    "            f'attr{i+1}': float(model.coef_[0][i]) \n",
    "            for i in range(N_ATTRIBUTES)\n",
    "        }\n",
    "        \n",
    "        return learned_weights\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting regression for {scenario['short_name']}: {e}\")\n",
    "        return {f'attr{i+1}': 0.0 for i in range(N_ATTRIBUTES)}\n",
    "\n",
    "print(\"✓ Logistic regression functions defined\")\n",
    "print(\"\\nRegression model: selection ~ d₁ + d₂ + d₃ + d₄ + d₅\")\n",
    "print(\"where d_i = (a_i - b_i) / (max_i - min_i)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c466713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate learned weights for all scenarios\n",
    "print(\"Estimating learned weights via logistic regression...\")\n",
    "print(\"This applies the method from Keeney & Raiffa (1993) referenced in the paper.\\n\")\n",
    "\n",
    "learned_weights = {}\n",
    "failed_scenarios = []\n",
    "\n",
    "for scenario in tqdm(scenarios[:N_INSTILLED_PREFERENCES]):\n",
    "    weights = estimate_learned_weights(validation_decisions, scenario)\n",
    "    learned_weights[scenario['short_name']] = weights\n",
    "    \n",
    "    # Check if all weights are zero (indicates failure)\n",
    "    if all(w == 0.0 for w in weights.values()):\n",
    "        failed_scenarios.append(scenario['short_name'])\n",
    "\n",
    "print(f\"\\n✓ Estimated weights for {len(learned_weights)} scenarios\")\n",
    "if failed_scenarios:\n",
    "    print(f\"⚠ Failed to estimate weights for {len(failed_scenarios)} scenarios (all decisions identical)\")\n",
    "\n",
    "# Save learned weights\n",
    "learned_weights_df = pd.DataFrame([\n",
    "    {\"scenario\": k, **v} for k, v in learned_weights.items()\n",
    "])\n",
    "learned_weights_file = Path(f\"data/{FINE_TUNED_MODEL}_learned_weights.csv\")\n",
    "learned_weights_df.to_csv(learned_weights_file, index=False)\n",
    "print(f\"✓ Saved learned weights to {learned_weights_file}\")\n",
    "\n",
    "# Display comparison for first scenario\n",
    "first_scenario = list(target_weights.keys())[0]\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"EXAMPLE: {first_scenario}\")\n",
    "print(\"=\"*70)\n",
    "print(\"Attribute |  Target Weight | Learned Weight | Difference\")\n",
    "print(\"-\"*70)\n",
    "for i in range(1, N_ATTRIBUTES + 1):\n",
    "    attr = f'attr{i}'\n",
    "    target = target_weights[first_scenario][attr]\n",
    "    learned = learned_weights[first_scenario][attr]\n",
    "    diff = abs(target - learned)\n",
    "    print(f\"  {attr}     |     {target:6.1f}     |     {learned:6.1f}     |   {diff:6.1f}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20fce0",
   "metadata": {},
   "source": [
    "## Step 7: VERIFY PREFERENCE TRAINING SUCCESS\n",
    "\n",
    "Compare target weights with learned weights to verify that fine-tuning was effective.\n",
    "\n",
    "**Success criterion from paper:** Correlation r > 0.90 indicates successful preference instillation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3db311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_target_vs_learned_weights(target_weights, learned_weights):\n",
    "    \"\"\"\n",
    "    Compare target and learned weights.\n",
    "    \n",
    "    This verifies that preference training was successful as described in the paper:\n",
    "    'we compared these learned weights with the target weights to verify that \n",
    "    the model had successfully internalized the target weights'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten weights for correlation analysis\n",
    "    target_flat = []\n",
    "    learned_flat = []\n",
    "    \n",
    "    for scenario_name in target_weights.keys():\n",
    "        if scenario_name in learned_weights:\n",
    "            for attr in ['attr1', 'attr2', 'attr3', 'attr4', 'attr5']:\n",
    "                target_flat.append(target_weights[scenario_name][attr])\n",
    "                learned_flat.append(learned_weights[scenario_name][attr])\n",
    "    \n",
    "    # Calculate Pearson correlation\n",
    "    correlation, p_value = pearsonr(target_flat, learned_flat)\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    mae = np.mean(np.abs(np.array(target_flat) - np.array(learned_flat)))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=\"*70)\n",
    "    print(\"PREFERENCE TRAINING VERIFICATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Correlation (r):        {correlation:.4f}\")\n",
    "    print(f\"p-value:                {p_value:.2e}\")\n",
    "    print(f\"Mean Absolute Error:    {mae:.2f}\")\n",
    "    print(f\"Number of comparisons:  {len(target_flat)} weights ({len(target_weights)} scenarios × 5 attributes)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Interpret results\n",
    "    print(\"\\nINTERPRETATION:\")\n",
    "    if correlation > 0.90:\n",
    "        print(\"✓ EXCELLENT: Preference training was highly successful (r > 0.90)\")\n",
    "        print(\"  The model has successfully internalized the target preference weights.\")\n",
    "    elif correlation > 0.75:\n",
    "        print(\"✓ GOOD: Preference training was successful (r > 0.75)\")\n",
    "        print(\"  The model learned the preferences reasonably well.\")\n",
    "    elif correlation > 0.50:\n",
    "        print(\"⚠ MODERATE: Preference training showed moderate success (r > 0.50)\")\n",
    "        print(\"  The model learned some preferences but not perfectly.\")\n",
    "    else:\n",
    "        print(\"✗ POOR: Preference training was not successful (r < 0.50)\")\n",
    "        print(\"  Consider: More training examples, longer fine-tuning, or different model.\")\n",
    "    \n",
    "    return correlation, p_value, mae\n",
    "\n",
    "# Perform comparison\n",
    "correlation, p_value, mae = compare_target_vs_learned_weights(target_weights, learned_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae52438",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "target_flat = []\n",
    "learned_flat = []\n",
    "\n",
    "for scenario_name in target_weights.keys():\n",
    "    if scenario_name in learned_weights:\n",
    "        for attr in ['attr1', 'attr2', 'attr3', 'attr4', 'attr5']:\n",
    "            target_flat.append(target_weights[scenario_name][attr])\n",
    "            learned_flat.append(learned_weights[scenario_name][attr])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(target_flat, learned_flat, alpha=0.5, s=30)\n",
    "plt.plot([-100, 100], [-100, 100], 'r--', linewidth=2, label='Perfect agreement')\n",
    "plt.xlabel('Target Weights (Ground Truth)', fontsize=14)\n",
    "plt.ylabel('Learned Weights (from Logistic Regression)', fontsize=14)\n",
    "plt.title(f'Preference Training Verification\\n(Pearson r = {correlation:.3f}, p < {p_value:.1e})', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([-110, 110])\n",
    "plt.ylim([-110, 110])\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.2, linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.2, linewidth=0.5)\n",
    "\n",
    "# Save plot\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "plot_file = results_dir / f\"{FINE_TUNED_MODEL}_weight_comparison.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved plot to {plot_file}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-attribute analysis\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for i in range(1, 6):\n",
    "    attr = f'attr{i}'\n",
    "    target_vals = [target_weights[s][attr] for s in target_weights.keys() if s in learned_weights]\n",
    "    learned_vals = [learned_weights[s][attr] for s in target_weights.keys() if s in learned_weights]\n",
    "    \n",
    "    axes[i-1].scatter(target_vals, learned_vals, alpha=0.5)\n",
    "    axes[i-1].plot([-100, 100], [-100, 100], 'r--', linewidth=1)\n",
    "    axes[i-1].set_xlabel('Target', fontsize=10)\n",
    "    axes[i-1].set_ylabel('Learned', fontsize=10)\n",
    "    axes[i-1].set_title(f'Attribute {i}', fontsize=12)\n",
    "    axes[i-1].grid(True, alpha=0.3)\n",
    "    axes[i-1].set_xlim([-110, 110])\n",
    "    axes[i-1].set_ylim([-110, 110])\n",
    "    \n",
    "    # Calculate per-attribute correlation\n",
    "    r, _ = pearsonr(target_vals, learned_vals)\n",
    "    axes[i-1].text(0.05, 0.95, f'r={r:.3f}', transform=axes[i-1].transAxes,\n",
    "                   verticalalignment='top', fontsize=10,\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / f\"{FINE_TUNED_MODEL}_per_attribute_comparison.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved per-attribute plot to {plot_file}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d8f55",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. ✓ Generated 100 decision scenarios with random target preference weights\n",
    "2. ✓ Created 5,000 training examples (50 per scenario) following target weights\n",
    "3. ✓ Fine-tuned model on these examples\n",
    "4. ✓ Generated 5,000 validation decisions with fine-tuned model\n",
    "5. ✓ **Estimated learned weights using logistic regression (Keeney & Raiffa, 1993)**\n",
    "6. ✓ **Verified preference training by comparing target vs. learned weights**\n",
    "7. ✓ Visualized results\n",
    "\n",
    "### Key Result\n",
    "\n",
    "**Correlation between target and learned weights:** r = {:.3f}\n",
    "\n",
    "This indicates the model has {} internalized the preference weights.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Experiment 1b: Test Introspection**\n",
    "- Ask the model to report the weights it believes it uses\n",
    "- Compare reported weights with learned weights\n",
    "- Expected baseline accuracy: r ≈ 0.30-0.50\n",
    "\n",
    "**Experiment 2: Introspection Training**\n",
    "- Fine-tune model to accurately report its weights\n",
    "- Test if introspection improves: r ≈ 0.70-0.85\n",
    "\n",
    "**Experiment 3: Generalization**\n",
    "- Test introspection on scenarios 100-200 (not seen during training)\n",
    "- Measure if introspection training generalizes to native preferences\n",
    "\n",
    "### References\n",
    "\n",
    "- Plunkett et al. (2025). Self-Interpretability. arXiv:2505.17120\n",
    "- Keeney & Raiffa (1993). Decisions with Multiple Objectives\n",
    "- GitHub: https://github.com/dillonplunkett/self-interpretability/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python SLI",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
